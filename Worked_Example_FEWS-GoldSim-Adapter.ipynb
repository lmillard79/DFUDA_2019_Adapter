{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Delft FEWS User Day 2019\n",
    "\n",
    "### by Lindsay Millard, Hydrologist Seqwater\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Jupyter Notebooks are fast way to develop and trial code for Delft FEWS adapters\n",
    "visit:\n",
    "https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks\n",
    "to learn more about Jupyter Notebooks\n",
    "Bring to your particular attention the \n",
    "####  Pandas for Data Analysis \n",
    "Example notebooks - such as Timeseries Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let start off by calling the following Python modules to run the code in this example. Most are standard libraries within a Python installation. <br> If your Anaconda Distribution (or vanilla Python) does not have it then you can use either conda or pip to install them.\n",
    "\n",
    "Command Line >>> Pip install 'missing module'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.0'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xml.etree.ElementTree import *  # Import everything \n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import os\n",
    "\n",
    "## from xlsxwriter.workbook import Workbook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd  ## Abbreviate it using the convention\n",
    "\n",
    "# Check what version of the module you are running, lets check Pandas.\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set up functions for checking the currency of the file found in the system path.  \n",
    "This needs to be run before the main part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Worker Function for checking currency\n",
    "def newest(path):\n",
    "    files = os.listdir(path)\n",
    "    paths = [os.path.join(path, basename) for basename in files]\n",
    "    return max(paths, key=os.path.getctime)\n",
    "\n",
    "# It is a good idea to put your variables up front\n",
    "fewsNamespace=\"http://www.wldelft.nl/fews/PI\"\n",
    "\n",
    "# The working folder root path can be defined here and then built on later when looking for files or output paths\n",
    "regionHome = r'W:\\500_Reference\\530_Presentations\\04_FEWS User Conference - Australia\\2019\\Python_LinguaFrancaofFEWS\\GoldSim/'\n",
    "\n",
    "regionHome = r'C:\\Users\\lmillard\\GitRepos\\DFUDA_2019_Adapter/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using For and If loops are a great way to pull data into the Python interpreter. It is probably a little confusing at first but the commentary along the way should make aspects a little clearer. If it doesn't make sense it can be unwrapped into a simple example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importState\n"
     ]
    }
   ],
   "source": [
    "# This is a list of the names of the XML files to go look for. I.e. look in regionHome for import.xml\n",
    "XMLs = ['importState','import']\n",
    "\n",
    "for xml in XMLs:\n",
    "    fin = os.path.join(regionHome,'1_Input',xml+'.xml')    \n",
    "    FEWS_Export = fin   #  regionHome+'1_Input/importState.xml'\n",
    "\n",
    "    parameterNames = ['HNPD_OUT','Reservoir.inflow.forecast', 'Reservoir.outflow.forecast', 'Gate.setting.forecast',]\n",
    "    spreadsheetNames = {'Reservoir.inflow.forecast':'LakeInflows', 'Reservoir.outflow.forecast':'RegulatorFlows', 'Gate.setting.forecast':'Gates', 'HNPD_OUT':'LakeLevels'}\n",
    "\n",
    "XMLs = ['importState']#,'import']\n",
    "## This will only work one by one in a Jupyter Notebook with many cells spilting the loop\n",
    "\n",
    "for xml in XMLs:\n",
    "    print(xml)\n",
    "    fin = os.path.join(regionHome,'1_Input',xml+'.xml')    \n",
    "    FEWS_Export = fin   #  regionHome+'1_Input/importState.xml'\n",
    "\n",
    "    parameterNames = ['HNPD_OUT','Reservoir.inflow.forecast', 'Reservoir.outflow.forecast', 'Gate.setting.forecast',]\n",
    "    spreadsheetNames = {'Reservoir.inflow.forecast':'LakeInflows', 'Reservoir.outflow.forecast':'RegulatorFlows', 'Gate.setting.forecast':'Gates', 'HNPD_OUT':'LakeLevels'}\n",
    "\n",
    "    for parametername in parameterNames:                   \n",
    "        ParList=[]\n",
    "        par =[]\n",
    "        locId=[]\n",
    "        i = 0    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is reminder of what an XML looks like (using Grid display in XML spy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/XML_timeseries.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code the will scrape timeseries data out of an XML file exported from FEWS.  \n",
    "The header information can be dealt with first. You can retrieve the number of Series (7), ParameterId (Reservoir.inflow.forecast) and MissingValue (-999.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have opended the following file:\n",
      "C:\\Users\\lmillard\\GitRepos\\DFUDA_2019_Adapter/1_Input\\importState.xml\n"
     ]
    }
   ],
   "source": [
    "        with open(FEWS_Export, \"r\") as file:\n",
    "            print('We have opended the following file:\\n'+FEWS_Export)\n",
    "            \n",
    "            tree = parse(file)\n",
    "            PItimeSeries = tree.getroot()\n",
    "            \n",
    "            series = PItimeSeries.findall('.//{' + fewsNamespace + '}series')\n",
    "            ## Search for all the Parameters defined in the XML\n",
    "            Parameters=PItimeSeries.findall('.//{' + fewsNamespace + '}parameterId')\n",
    "            for param in Parameters:\n",
    "                ParList.append(param.text)\n",
    "            \n",
    "            parCount=0  # Initialise a counter of parameters found                \n",
    "            \n",
    "            missingvalue = PItimeSeries.findall('.//{' + fewsNamespace + '}missVal')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code does not know how many timeseries are in the XML. The following code will loop through all of the parameters of interest and then count the length.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Timestep rows:     481\n",
      "Number of Parameter columns: 0\n"
     ]
    }
   ],
   "source": [
    "            for S in series:\n",
    "                events = S.findall('.//{' + fewsNamespace + '}event')\n",
    "                \n",
    "                # When working with Date Time we will need to define it is a DateTime not a string or float\n",
    "                ArrayDates = np.zeros((len(events)),dtype='datetime64[ns]')\n",
    "                \n",
    "                par = S.find('.//{' + fewsNamespace + '}parameterId').text\n",
    "                if par==parametername:\n",
    "                    parCount = parCount + 1\n",
    "            \n",
    "            ## Once you loop through the file you are able to set up empty array of appropriate size            \n",
    "            ArrayValues = np.zeros((len(events), parCount))\n",
    "            print('Number of Timestep rows:     '+ str(ArrayValues.shape[0]))\n",
    "            print('Number of Parameter columns: '+ str(ArrayValues.shape[1]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "            j = 0\n",
    "\n",
    "            DateList = []\n",
    "            for S in series:\n",
    "                par = S.find('.//{' + fewsNamespace + '}parameterId').text\n",
    "                \n",
    "                if par!=parametername:\n",
    "                    pass\n",
    "                \n",
    "                else:            \n",
    "                    events = S.findall('.//{' + fewsNamespace + '}event')\n",
    "                    locs = S.findall('.//{' + fewsNamespace + '}locationId')\n",
    "                    \n",
    "                    i=0\n",
    "                    for l in locs:\n",
    "                        locId.append(l.text)        \n",
    "                \n",
    "                    for ev in events:\n",
    "                        if ev.attrib['value'] == S.find('.//{' + fewsNamespace + '}missVal').text:\n",
    "                            ArrayValues[i,j] = float(0)\n",
    "                        else:\n",
    "                            ArrayValues[i,j] = float(ev.attrib['value'])\n",
    "                        \n",
    "                        strucTime_Tuple = datetime.strptime(ev.attrib['date'] + \" \" + ev.attrib['time'],\"%Y-%m-%d %H:%M:%S\")\n",
    "                        ArrayDates[i] = strucTime_Tuple\n",
    "\n",
    "                        i += 1\n",
    "                    j += 1\n",
    "            row = i\n",
    "            col = j\n",
    "            print(row,col)\n",
    "\n",
    "            DFd = pd.DataFrame(ArrayDates) \n",
    "            # Rename the columns\n",
    "            DFd.columns = ['DateTime']\n",
    "            \n",
    "            DFv = pd.DataFrame(ArrayValues) \n",
    "            # Rename the columns\n",
    "            DFv.columns = locId\n",
    "            \n",
    "            # Concatenate both columns together, do this column-wise (axis=1)\n",
    "            DF = pd.concat([DFd,DFv],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate.setting.forecast\n",
      "Gate.setting.forecast\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DateTime\n",
       "0 1970-01-01\n",
       "1 1970-01-01\n",
       "2 1970-01-01\n",
       "3 1970-01-01\n",
       "4 1970-01-01"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        ## Because the list of files and parameters contradict, we need to stop it overwriting the previous loop\n",
    "        if xml == 'importState' and parametername == 'HNPD_OUT':\n",
    "            GoldSIMSpreadSheet = regionHome+'2_Model/'+spreadsheetNames[parametername]+'.xlsx'\n",
    "\n",
    "            #Let us use Pandas to write the file to a spreadsheet, this can be done in one line\n",
    "            DF.to_excel(GoldSIMSpreadSheet,sheet_name='FEWS_Export')\n",
    "            print(parametername)\n",
    "\n",
    "        elif parametername != 'HNPD_OUT':\n",
    "            GoldSIMSpreadSheet = regionHome+'2_Model/'+spreadsheetNames[parametername]+'.xlsx'\n",
    "            DF.to_excel(GoldSIMSpreadSheet,sheet_name='FEWS_Export')\n",
    "            print(parametername)\n",
    "\n",
    "print(parametername)\n",
    "DF.head()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Run the Pre- Adapter\n",
    "\n",
    "## Run the Master For loop from the top\n",
    "\n",
    "This will allow the file to iterate over all of the parameters and files of interest.  \n",
    "This will produce the XLSX files that GoldSim requires to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "XMLs = ['importState','import']\n",
    "\n",
    "for xml in XMLs:\n",
    "    fin = os.path.join(regionHome,'1_Input',xml+'.xml')    \n",
    "    FEWS_Export = fin   #  regionHome+'1_Input/importState.xml'\n",
    "\n",
    "    parameterNames = ['HNPD_OUT','Reservoir.inflow.forecast', 'Reservoir.outflow.forecast', 'Gate.setting.forecast',]\n",
    "    spreadsheetNames = {'Reservoir.inflow.forecast':'LakeInflows', 'Reservoir.outflow.forecast':'RegulatorFlows', 'Gate.setting.forecast':'Gates', 'HNPD_OUT':'LakeLevels'}\n",
    "\n",
    "    for parametername in parameterNames:                   \n",
    "        ParList=[]\n",
    "        par =[]\n",
    "        locId=[]\n",
    "        i = 0    \n",
    "        \n",
    "        with open(FEWS_Export, \"r\") as file:\n",
    "            tree = parse(file)\n",
    "            PItimeSeries = tree.getroot()\n",
    "            Parameters=PItimeSeries.findall('.//{' + fewsNamespace + '}parameterId')\n",
    "            for param in Parameters:\n",
    "                ParList.append(param.text)\n",
    "                \n",
    "            missingvalue = PItimeSeries.findall('.//{' + fewsNamespace + '}missVal')\n",
    "            series = PItimeSeries.findall('.//{' + fewsNamespace + '}series')\n",
    "\n",
    "            # determine size of aray needed. all series MUST be the same length\n",
    "            parCount=0\n",
    "            for S in series:\n",
    "                events = S.findall('.//{' + fewsNamespace + '}event')\n",
    "                ArrayDates = np.zeros((len(events)),dtype='datetime64[ns]')\n",
    "                \n",
    "                par = S.find('.//{' + fewsNamespace + '}parameterId').text\n",
    "                if par==parametername:\n",
    "                    parCount = parCount + 1\n",
    "            \n",
    "            ArrayValues = np.zeros((len(events), parCount))\n",
    "            j = 0\n",
    "\n",
    "            DateList = []\n",
    "            for S in series:\n",
    "                par = S.find('.//{' + fewsNamespace + '}parameterId').text\n",
    "                \n",
    "                if par!=parametername:\n",
    "                    pass\n",
    "                \n",
    "                else:            \n",
    "                    events = S.findall('.//{' + fewsNamespace + '}event')\n",
    "                    locs = S.findall('.//{' + fewsNamespace + '}locationId')\n",
    "                    i=0\n",
    "                    for l in locs:\n",
    "                        locId.append(l.text)        \n",
    "                \n",
    "                    for ev in events:\n",
    "                        if ev.attrib['value'] == S.find('.//{' + fewsNamespace + '}missVal').text:\n",
    "                            ArrayValues[i,j] = float(0)\n",
    "                        else:\n",
    "                            ArrayValues[i,j] = float(ev.attrib['value'])\n",
    "                        \n",
    "                        strucTime_Tuple = datetime.strptime(ev.attrib['date'] + \" \" + ev.attrib['time'],\"%Y-%m-%d %H:%M:%S\")\n",
    "                        ArrayDates[i] = strucTime_Tuple\n",
    "\n",
    "                        i += 1\n",
    "                    j +=  1\n",
    "            row = i\n",
    "            col = j\n",
    "\n",
    "            DFd = pd.DataFrame(ArrayDates) \n",
    "            DFd.columns = ['DateTime']\n",
    "            DFv = pd.DataFrame(ArrayValues) \n",
    "            DFv.columns = locId\n",
    "\n",
    "            DF = pd.concat([DFd,DFv],axis=1)\n",
    "                \n",
    "            \n",
    "        if xml == 'importState' and parametername == 'HNPD_OUT':\n",
    "            GoldSIMSpreadSheet = regionHome+'2_Model/'+spreadsheetNames[parametername]+'.xlsx'\n",
    "            DF.to_excel(GoldSIMSpreadSheet,sheet_name='FEWS_Export')\n",
    "        \n",
    "        elif parametername != 'HNPD_OUT':\n",
    "            GoldSIMSpreadSheet = regionHome+'2_Model/'+spreadsheetNames[parametername]+'.xlsx'\n",
    "            DF.to_excel(GoldSIMSpreadSheet,sheet_name='FEWS_Export')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Run GoldSim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Excel spreadsheet files for the:\n",
    "    Gates, LakeInflows, LakeLevels and RegulatorFlows have all been produced by Part 1.\n",
    "    \n",
    "Running the GoldSim Player file produces RoutedFlows.txt and RoutedLevels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/GoldSim_RunTheTimeseries.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Run the Post-Adapter\n",
    "\n",
    "Using the RoutedFlows.txt and RoutedLevel.txt we can now turn them back into XML timeseries for FEWS to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
